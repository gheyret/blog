谷歌机器学习速成课程学习简要总结

之前大致的看了一些该系列课程，然后看了吴老师和其他机器学习教程或书籍，最近想再系统性的把所有知识全部过滤一遍，所以再次将此速成课程看一遍，一来加深自己的理解，并把不清楚的地方搞清楚。此速成课程浅显易懂，在学完之后想对其进行一个总结和提炼，达到书越读越薄的效果。

### 什么是机器学习？以及它的一些术语 ###

机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测，可分为监督和无监督学习。

标签： 指我们要预测的事物，即简单线性回归中的 y 变量。  
特征： 是输入变量，即线性回归中的 x 变量。  
样本： 指数据特定实例：x   可分为有标签样本和无标签样本。  
模型： 模型定义了特征与标签之前的关系。  **训练**表示创建或学习模型； **推断**表示将训练后的模型应用于无标签样本。  
回归与分类： **回归**模型可预测连续值。  **分类**模型可预测离散值。

### 线性回归 ###

y' = b + w_1x_1  
y' = b + w_1x_1 + w_2x_2 + w_3x_3

平方损失（L2损失） 

![](https://i.imgur.com/WsttpTz.png)

### 降低损失 ###

**迭代方法**

![用于训练模型的迭代方法](https://i.imgur.com/apOMDs1.png)

**梯度下降法**

![](https://i.imgur.com/R0ez6IG.png)

**学习速率**

![](https://i.imgur.com/Xpv22ZH.png)

一维空间中的理想学习速率是 1/f(x)″（f(x) 对 x 的二阶导数的倒数）。
二维或多维空间中的理想学习速率是海森矩阵（由二阶偏导数组成的矩阵）的倒数。
广义凸函数的情况则更为复杂。

**随机梯度下降法**

在梯度下降法中，批量指的是用于在单次迭代中计算梯度的样本总数。包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

梯度下降法也适用于包含多个特征的特征集。

### 泛化 ###

当我们在降低损失时，有可能会存在过拟合的风险，即模型对我们的样本数据拟合得很好，但是对新的数据表现很糟糕。如何预防这一问题呢，可将数据集分成**训练集**和**测试集**。请勿对测试数据进行训练。 如果您的评估指标取得了意外的好结果，则可能表明您不小心对测试集进行了训练。例如，高准确率可能表明测试数据泄露到了训练集。

![](https://i.imgur.com/J4tFCZQ.png)

机器学习细则：

- 我们从分布中随机抽取独立同分布 (i.i.d) 的样本。换言之，样本之间不会互相影响。（另一种解释：i.i.d. 是表示变量随机性的一种方式）。
- 分布是平稳的；即分布在数据集内不会发生变化。
- 我们从同一分布的数据划分中抽取样本。

**另一个划分**

![](https://i.imgur.com/dNnRwlY.png)

![](https://i.imgur.com/uo7EiGf.png)

### 表示法 ###

**特征工程**

**独热编码 (one-hot encoding)**

一种稀疏向量，其中：

- 一个元素设为 1。
- 所有其他元素均设为 0。

独热编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为独热向量，向量的大小为 15000。

**良好特征的特点**

- 避免很少的离散特征值
- 最好具有清晰明确的含义
- 不要将“神奇”的值与实际数据混为一谈
- 考虑上游不稳定性

**数据清理**

**特征缩放**是指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。如果某个特征集只包含一个特征，则缩放可以提供的实际好处微乎其微或根本没有。不过，如果特征集包含多个特征，则缩放特征可以带来以下优势：

- 帮助梯度下降法更快速地收敛。
- 帮助避免“NaN 陷阱”。在这种陷阱中，模型中的一个数值变成 NaN（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 NaN。
- 帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力。

**处理极端离群值**

如果数据中存在极端的数据情况，则可将这些极端值限制在一定的范围。

**分箱**

在数据集中，latitude 是一个浮点值。不过，在我们的模型中将 latitude 表示为浮点特征没有意义。这是因为纬度和房屋价值之间不存在线性关系。例如，纬度 35 处的房屋并不比纬度 34 处的房屋贵 35/34（或更便宜）。但是，纬度或许能很好地预测房屋价值。为了将纬度变为一项实用的预测指标，我们对纬度“分箱”。

![](https://i.imgur.com/KsjYBzR.png)

**清查**

在现实生活中，数据集中的很多样本是不可靠的，原因有以下一种或多种：

- 遗漏值。 例如，有人忘记为某个房屋的年龄输入值。
- 重复样本。 例如，服务器错误地将同一条记录上传了两次。
- 不良标签。 例如，有人错误地将一颗橡树的图片标记为枫树。
- 不良特征值。 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。

### 特征组合 ###

线性学习器可以很好地扩展到大量数据。对大规模数据集使用特征组合是学习高度复杂模型的一种有效策略。神经网络可提供另一种策略。

### L2正则化 ###

**正则化**指的是降低模型的复杂度以减少过拟合

![](https://i.imgur.com/uJWcJlP.png)

上图显示的是某个模型的训练损失逐渐减少，但验证损失最终增加。换言之，该泛化曲线显示该模型与训练集中的数据过拟合。根据奥卡姆剃刀定律，或许我们可以通过降低复杂模型的复杂度来防止过拟合，这种原则称为正则化。也就是说，并非只是以最小化损失（经验风险最小化）为目标：

minimize(Loss(Data|Model))

而是以最小化损失和复杂度为目标，这称为**结构风险最小化**：

minimize(Loss(Data|Model) + complexity(Model))

现在，我们的训练优化算法是一个由两项内容组成的函数：一个是损失项，用于衡量模型与数据的拟合度，另一个是正则化项，用于衡量模型复杂度。

两种衡量模型复杂度的常见方式:

- 将模型复杂度作为模型中所有特征的权重的函数。
- 将模型复杂度作为具有非零权重的特征总数的函数。

如果模型复杂度是权重的函数，则特征权重的绝对值越高，对模型复杂度的贡献就越大。我们可以使用 L2 正则化公式来量化复杂度，该公式将正则化项定义为所有特征权重的平方和：

![](https://i.imgur.com/1B9QU6y.png)

在这个公式中，接近于 0 的权重对模型复杂度几乎没有影响，而离群值权重则可能会产生巨大的影响。

**Lambda**

模型开发者通过以下方式来调整正则化项的整体影响：用正则化项的值乘以名为 lambda（又称为正则化率）的标量。执行 L2 正则化对模型具有以下影响

- 使权重值接近于 0（但并非正好为 0）
- 使权重的平均值接近于 0，且呈正态（钟形曲线或高斯曲线）分布。

增加 lambda 值将增强正则化效果。 例如，lambda 值较高的权重直方图可能会如下图所示：

![](https://i.imgur.com/sWYOumu.png)

降低 lambda 的值往往会得出比较平缓的直方图，如下图所示：

![](https://i.imgur.com/utMCtCr.png)

在选择 lambda 值时，目标是在简单化和训练数据拟合之间达到适当的平衡：

- 如果您的 lambda 值过高，则模型会非常简单，但是您将面临数据欠拟合的风险。您的模型将无法从训练数据中获得足够的信息来做出有用的预测。
- 如果您的 lambda 值过低，则模型会比较复杂，并且您将面临数据过拟合的风险。您的模型将因获得过多训练数据特点方面的信息而无法泛化到新数据。

理想的 lambda 值生成的模型可以很好地泛化到以前未见过的新数据。 遗憾的是，理想的 lambda 值取决于数据，因此您需要手动或自动进行一些调整。

### 逻辑回归 ###

**计算概率**

**S 型函数**定义如下：

![](https://i.imgur.com/qAOBDxx.png)

- y' 是逻辑回归模型针对特定样本的输出。
- z 是 b + w1x1 + w2x2 + … wNxN
- “w”值是该模型学习的权重和偏差。
- “x”值是特定样本的特征值。

S 型函数会产生以下曲线图：

![](https://i.imgur.com/f2QvI0c.png)

**模型训练**

线性回归的损失函数是平方损失。逻辑回归的损失函数是对数损失函数，定义如下：

![](https://i.imgur.com/DaVWll1.png)

其中：

- (xy)ϵD 是包含很多有标签样本 (x,y) 的数据集。
- “y”是有标签样本中的标签。由于这是逻辑回归，因此“y”的每个值必须是 0 或 1。
- “y'”是对于特征集“x”的预测值（介于 0 和 1 之间）。
- 对数损失函数的方程式与 Shannon 信息论中的熵测量密切相关。它也是似然函数的负对数（假设“y”属于伯努利分布）。实际上，最大限度地降低损失函数的值会生成最大的似然估计值。

正则化在逻辑回归建模中极其重要。如果没有正则化，逻辑回归的渐近性会不断促使损失在高维度空间内达到 0。因此，大多数逻辑回归模型会使用以下两个策略之一来降低模型复杂性：

- L2 正则化。
- 早停法，即，限制训练步数或学习速率。
- L1 正则化。

假设您向每个样本分配一个唯一 ID，且将每个 ID 映射到其自己的特征。如果您未指定正则化函数，模型会变得完全过拟合。这是因为模型会尝试促使所有样本的损失达到 0 但始终达不到，从而使每个指示器特征的权重接近正无穷或负无穷。当有大量罕见的特征组合且每个样本中仅一个时，包含特征组合的高维度数据会出现这种情况。幸运的是，使用 L2 或早停法可以防止出现此类问题。

### 分类 ###

为了将逻辑回归值映射到二元类别，您必须指定分类阈值（也称为判定阈值）。如果值高于该阈值，则表示“垃圾邮件”；如果值低于该阈值，则表示“非垃圾邮件”。人们往往会认为分类阈值应始终为 0.5，但阈值取决于具体问题，因此您必须对其进行调整。

**真正例TP**是指模型将正类别样本正确地预测为正类别。同样，**真负例FP**是指模型将负类别样本正确地预测为负类别。
**假正例FN**是指模型将负类别样本错误地预测为正类别，而**假负例TN**是指模型将正类别样本错误地预测为负类别。

准确率 = (TP + TN) / (TP + TN + FP + FN)

您使用分类不平衡的数据集（比如正类别标签和负类别标签的数量之间存在明显差异）时，单单准确率一项并不能反映全面情况。

精确率 = TP / (TP + FP)

召回率 = TP / (TP + FN)

ROC 曲线（接收者操作特征曲线）是一种显示分类模型在所有分类阈值下的效果的图表。该曲线绘制了以下两个参数：

- 真正例率 = 召回率 = TP / (TP + FN)
- 假正例率 = FP / (FP + TN)

ROC 曲线用于绘制采用不同分类阈值时的 TPR 与 FPR。降低分类阈值会导致将更多样本归为正类别，从而增加假正例和真正例的个数。下图显示了一个典型的 ROC 曲线。

![](https://i.imgur.com/CAKjB1G.png)

为了计算 ROC 曲线上的点，我们可以使用不同的分类阈值多次评估逻辑回归模型，但这样做效率非常低。幸运的是，有一种基于排序的高效算法可以为我们提供此类信息，这种算法称为曲线下面积。

曲线下面积表示“ROC 曲线下面积”。也就是说，曲线下面积测量的是从 (0,0) 到 (1,1) 之间整个 ROC 曲线以下的整个二维面积。













